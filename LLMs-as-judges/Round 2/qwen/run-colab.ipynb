{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andradeM17/Data-quality/blob/main/LLMs-as-judges/Round%202/gpt-oss/run-colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj6KvThm8Jjn"
      },
      "source": [
        "# Run OpenAI gpt-oss 20B in a FREE Google Colab\n",
        "\n",
        "OpenAI released `gpt-oss` [120B](https://hf.co/openai/gpt-oss-120b) and [20B](https://hf.co/openai/gpt-oss-20b). Both models are Apache 2.0 licensed.\n",
        "\n",
        "Specifically, `gpt-oss-20b` was made for lower latency and local or specialized use cases (21B parameters with 3.6B active parameters).\n",
        "\n",
        "Since the models were trained in native MXFP4 quantization it makes it easy to run the 20B even in resource constrained environments like Google Colab.\n",
        "\n",
        "Authored by: [Pedro](https://huggingface.co/pcuenq) and [VB](https://huggingface.co/reach-vb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv2foJJa9Xkc"
      },
      "source": [
        "## Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gUEKrLEvJmf"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade torch==2.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3N00UT7gtpkp"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers triton==3.4 kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GW0knW2w3ND"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -q torchvision torchaudio -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxU0WKwtH19m"
      },
      "source": [
        "Please, restart your Colab runtime session after installing the packages above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3xCxY159frD"
      },
      "source": [
        "## Load the model and data\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "We load the model from here: [openai/gpt-oss-20b](https://hf.co/openai/gpt-oss-20b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2HFwdkXu2R1"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"openai/gpt-oss-20b\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"cuda\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "f-tmmhxhasRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91f6aaf8"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "llm_data = pd.read_csv('/content/drive/MyDrive/llmj-data.csv')\n",
        "print(\"llmdata.csv loaded successfully.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksxo7bjR_-th"
      },
      "source": [
        "## Specify Reasoning Effort"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcv6QdcQKLr0"
      },
      "source": [
        "Simply pass it as an additional argument to `apply_chat_template()`. Supported values are `\"low\"`, `\"medium\"` (default), or `\"high\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmnkAle608Hl"
      },
      "outputs": [],
      "source": [
        "guidelines= '''You will be given a text, containing two parts\n",
        "IF the first sample is \"nan\", it is a monolingual sample; ELSE the sample is parallel data.\n",
        "Using the guidelines, give the correct code for the text: NL, WL, X, CS, CB, or CC. Finish your statement in the format:\n",
        "'\\n\n",
        "Annotation: [value]\n",
        "\\n\\n';\n",
        "\n",
        "\n",
        "Guidelines:\n",
        "  IF the content is NOT linguistic OR you are unsure THEN annotate as NL NOTE: \"\"No or unsure\"\" → NL\n",
        "  ELSE IF the text is parallel data AND the target side is NOT in Modern (standardised) Irish OR the source side is NOT in Modern English THEN annotate as WL NOTE: \"\"No or Unsure\"\" → WL NOTE: Untranslated named entities on either side are treated as belonging to that language (e.g., \"\"Capnat\"\" on the English side counts as English) NOTE: If text is entirely named entities, it is more likely to be ‘Yes’\n",
        "  ELSE IF the Irish target text is NOT a direct translation of the English source text THEN annotate as X NOTE: \"\"Yes or unsure\"\" → next question; if not, annotate X\n",
        "  ELSE IF the Irish text is short (just headings, single unrelated phrases, OR five words or fewer) THEN annotate as CS NOTE: Short text = CS\n",
        "  ELSE IF the text is boilerplate OR low quality (text that would repeat across similar webpages, unnatural code-switching, unnatural phrasing, frequent unnatural formatting, misalignments) THEN annotate as CB NOTE: CB includes text not representative of natural language One formatting problem alone is NOT enough; more than one may qualify\n",
        "  ELSE annotate as CC NOTE: CC = natural language, not short, not boilerplate, properly aligned.'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for index, row in llm_data.iterrows():\n",
        "    # Assuming the first two columns (index 0 and 1) contain the text\n",
        "    # and need to be formatted as '\"col1_content\", \"col2_content\"'\n",
        "    text = f'\"{str(row.iloc[0])}\", \"{str(row.iloc[1])}\"'\n",
        "    print(f\"\\n{text}\\n\\n\")\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": guidelines},\n",
        "        {\"role\": \"user\", \"content\": text},\n",
        "    ]\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "        return_dict=True,\n",
        "        reasoning_effort=\"low\",\n",
        "    ).to(model.device)\n",
        "\n",
        "    generated = model.generate(**inputs, max_new_tokens=500)\n",
        "    decoded_output = tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[-1]:])\n",
        "\n",
        "    start_marker = \"<|start|>assistant<|channel|>final<|message|>\"\n",
        "    end_marker = \"<|return|>\"\n",
        "\n",
        "    start_index = decoded_output.find(start_marker)\n",
        "    if start_index != -1:\n",
        "        end_index = decoded_output.find(end_marker, start_index + len(start_marker))\n",
        "        if end_index != -1:\n",
        "            extracted_text = decoded_output[start_index + len(start_marker):end_index].strip()\n",
        "            print(extracted_text)\n",
        "        else:\n",
        "            print(\"No annotation\")\n",
        "    else:\n",
        "        print(\"No annotation\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}