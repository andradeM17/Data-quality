{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andradeM17/Data-quality/blob/main/LLMs-as-judges/Round%202/gpt-oss/run-colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj6KvThm8Jjn"
      },
      "source": [
        "# Mistral-7B-Instruct-v0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3xCxY159frD"
      },
      "source": [
        "We load the model from here: [mistralai/Mistral-7B-Instruct-v0.2](https://hf.co/mistralai/Mistral-7B-Instruct-v0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2HFwdkXu2R1"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"cuda\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "f-tmmhxhasRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91f6aaf8"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "llm_data = pd.read_csv('/content/drive/MyDrive/llmj-data.csv')\n",
        "print(\"llmdata.csv loaded successfully.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksxo7bjR_-th"
      },
      "source": [
        "## Specify Reasoning Effort"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcv6QdcQKLr0"
      },
      "source": [
        "Simply pass it as an additional argument to `apply_chat_template()`. Supported values are `\"low\"`, `\"medium\"` (default), or `\"high\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmnkAle608Hl"
      },
      "outputs": [],
      "source": [
        "guidelines= '''You will be given a text, containing two parts\n",
        "IF the first sample is \"nan\", it is a monolingual sample; ELSE the sample is parallel data.\n",
        "Using the guidelines, give the correct code for the text: NL, WL, X, CS, CB, or CC. Finish your statement in the format:\n",
        "'\\n\n",
        "Annotation: [value]\n",
        "\\n\\n';\n",
        "\n",
        "\n",
        "Guidelines:\n",
        "  IF the content is NOT linguistic OR you are unsure THEN annotate as NL NOTE: \"\"No or unsure\"\" → NL\n",
        "  ELSE IF the text is parallel data AND the target side is NOT in Modern (standardised) Irish OR the source side is NOT in Modern English THEN annotate as WL NOTE: \"\"No or Unsure\"\" → WL NOTE: Untranslated named entities on either side are treated as belonging to that language (e.g., \"\"Capnat\"\" on the English side counts as English) NOTE: If text is entirely named entities, it is more likely to be ‘Yes’\n",
        "  ELSE IF the Irish target text is NOT a direct translation of the English source text THEN annotate as X NOTE: \"\"Yes or unsure\"\" → next question; if not, annotate X\n",
        "  ELSE IF the Irish text is short (just headings, single unrelated phrases, OR five words or fewer) THEN annotate as CS NOTE: Short text = CS\n",
        "  ELSE IF the text is boilerplate OR low quality (text that would repeat across similar webpages, unnatural code-switching, unnatural phrasing, frequent unnatural formatting, misalignments) THEN annotate as CB NOTE: CB includes text not representative of natural language One formatting problem alone is NOT enough; more than one may qualify\n",
        "  ELSE annotate as CC NOTE: CC = natural language, not short, not boilerplate, properly aligned.'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for index, row in llm_data.iterrows():\n",
        "    text = f'\"{str(row.iloc[0])}\", \"{str(row.iloc[1])}\"'\n",
        "    print(f\"\\n{text}\\n\\n\")\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": guidelines},\n",
        "        {\"role\": \"user\", \"content\": text},\n",
        "    ]\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "        return_dict=True,\n",
        "        reasoning_effort=\"low\",\n",
        "    ).to(model.device)\n",
        "\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    generated = model.generate(**inputs, max_new_tokens=500)\n",
        "    decoded_output = tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
        "    decoded_output = decoded_output.strip()\n",
        "\n",
        "    print(index+1, decoded_output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}