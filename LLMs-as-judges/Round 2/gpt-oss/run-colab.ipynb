{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGfI8meEHXfM"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openai/openai-cookbook/blob/main/articles/gpt-oss/run-colab.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj6KvThm8Jjn"
      },
      "source": [
        "# Run OpenAI gpt-oss 20B in a FREE Google Colab\n",
        "\n",
        "OpenAI released `gpt-oss` [120B](https://hf.co/openai/gpt-oss-120b) and [20B](https://hf.co/openai/gpt-oss-20b). Both models are Apache 2.0 licensed.\n",
        "\n",
        "Specifically, `gpt-oss-20b` was made for lower latency and local or specialized use cases (21B parameters with 3.6B active parameters).\n",
        "\n",
        "Since the models were trained in native MXFP4 quantization it makes it easy to run the 20B even in resource constrained environments like Google Colab.\n",
        "\n",
        "Authored by: [Pedro](https://huggingface.co/pcuenq) and [VB](https://huggingface.co/reach-vb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv2foJJa9Xkc"
      },
      "source": [
        "## Setup environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMRXDOpY1Q3Q"
      },
      "source": [
        "Since support for mxfp4 in transformers is bleeding edge, we need a recent version of PyTorch and CUDA, in order to be able to install the `mxfp4` triton kernels.\n",
        "\n",
        "We also need to install transformers from source, and we uninstall `torchvision` and `torchaudio` to remove dependency conflicts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gUEKrLEvJmf"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade torch==2.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3N00UT7gtpkp"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers triton==3.4 kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GW0knW2w3ND"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -q torchvision torchaudio -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxU0WKwtH19m"
      },
      "source": [
        "Please, restart your Colab runtime session after installing the packages above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3xCxY159frD"
      },
      "source": [
        "## Load the model from Hugging Face in Google Colab\n",
        "\n",
        "We load the model from here: [openai/gpt-oss-20b](https://hf.co/openai/gpt-oss-20b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2HFwdkXu2R1"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"openai/gpt-oss-20b\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"cuda\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksxo7bjR_-th"
      },
      "source": [
        "## Specify Reasoning Effort"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcv6QdcQKLr0"
      },
      "source": [
        "Simply pass it as an additional argument to `apply_chat_template()`. Supported values are `\"low\"`, `\"medium\"` (default), or `\"high\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmnkAle608Hl"
      },
      "outputs": [],
      "source": [
        "guidelines= '''You will be given a text.\n",
        "If there are two samples, the first is English and the second is Irish. If you are given one, the text is Irish.\n",
        "Using the guidelines, give the correct code for the text: NL, WL, X, CS, CB, or CC. Finish your statement in the format 'Annotation: [value]';\n",
        "\n",
        "\n",
        "Guidelines: IF the content is NOT linguistic OR you are unsure THEN annotate as NL NOTE: \"\"No or unsure\"\" → NL ELSE IF the text is parallel data AND the target side is NOT in Modern (standardised) Irish OR the source side is NOT in Modern English THEN annotate as WL NOTE: \"\"No or Unsure\"\" → WL NOTE: Untranslated named entities on either side are treated as belonging to that language (e.g., \"\"Capnat\"\" on the English side counts as English) NOTE: If text is entirely named entities, it is more likely to be ‘Yes’ ELSE IF the Irish target text is NOT a direct translation of the English source text THEN annotate as X NOTE: \"\"Yes or unsure\"\" → next question; if not, annotate X ELSE IF the Irish text is short (just headings, single unrelated phrases, OR five words or fewer) THEN annotate as CS NOTE: Short text = CS ELSE IF the text is boilerplate OR low quality (text that would repeat across similar webpages, unnatural code-switching, unnatural phrasing, frequent unnatural formatting, misalignments) THEN annotate as CB NOTE: CB includes text not representative of natural language One formatting problem alone is NOT enough; more than one may qualify ELSE annotate as CC NOTE: CC = natural language, not short, not boilerplate, properly aligned.'''\n",
        "\n",
        "\n",
        "texts = ['\"\", \"Tarluithe Diat Worms Tógadh Taigh BhroughtonI gCille Chùithbeirt, Alba. Breitheanna 19 Feabhra — Luigi Boccherini, cumadóir ceoil is dordveidhleadóir Iodálach 24 Feabhra — Joseph Banks, luibheolaí Sasanach (b. 1820) 13 Aibreán — Thomas Jefferson 3ú uachtarán na Stát Aontaithe Mheiriceá (b. 1826) 24 Aibreán — Edmund Cartwright, ceapadóir Sasanach (b. 1823) 20 Bealtaine — Toussaint Louverture, ceannaire réabhlóideach Háitíoch (b. 1803) 24 Bealtaine — Jean-Paul Marat, idé-eolaí agus tíoránach cruálach le linn Réabhlóid na Fraince 26 Lúnasa — Antoine Lavoisier, ceimiceoir Francach Timpeall na bliana seo – Eibhlín Dubh Ní Chonaill, file Éireannach (b. 1800) 1 Nollaig- Martin Heinrich Klaproth, poitigéir Gearmánach Básanna 2 Iúil  — Spencer Compton, príomh-aire na Breataine Blianta\"',\n",
        "         '\"Healthcare Business Valuations\", \"dtugtar Cailín Kim watch gan\"',\n",
        "         '\"Within the ceiling defined above, it shall calculate the amount to be actually levied in such a way that the Government subsidies to be actually paid shall be at least equal to that amount;\", \"Laistigh den uasteorainn ala sainithe thuas, déanfaidh sé méid an tobhaigh iarbhir a riomh ar dhóigh go mbeidh na fóirdheontais Rialtais a íocfar iarbhír cóímhéid leis an tobhach sin, ar an gcuid is lú de.\"']\n",
        "\n",
        "\n",
        "for text in texts:\n",
        "  messages = [\n",
        "      {\"role\": \"system\", \"content\": guidelines},\n",
        "      {\"role\": \"user\", \"content\": text},\n",
        "  ]\n",
        "\n",
        "  inputs = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      add_generation_prompt=True,\n",
        "      return_tensors=\"pt\",\n",
        "      return_dict=True,\n",
        "      reasoning_effort=\"low\",\n",
        "  ).to(model.device)\n",
        "\n",
        "  generated = model.generate(**inputs, max_new_tokens=500)\n",
        "  print(tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[-1]:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf2-ocGqEC_r"
      },
      "source": [
        "## Try out other prompts and ideas!\n",
        "\n",
        "Check out our blogpost for other ideas: [https://hf.co/blog/welcome-openai-gpt-oss](https://hf.co/blog/welcome-openai-gpt-oss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QrnTpcCKd_n"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}